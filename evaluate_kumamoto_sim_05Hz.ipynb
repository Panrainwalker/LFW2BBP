{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60369cd5-784d-47f0-a5a3-ea20b565f125",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import torch\n",
    "from scipy.signal import stft\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "from torch.optim.lr_scheduler import ExponentialLR,StepLR,CosineAnnealingLR\n",
    "import scipy.signal as signal\n",
    "import pywt\n",
    "import random\n",
    "from scipy.signal import butter, filtfilt, detrend, lfilter\n",
    "import gc\n",
    "# import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4ab413-977d-4a88-b7a6-19e64b213c43",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac03aca-325e-4250-9acf-16a9329e3336",
   "metadata": {},
   "source": [
    "### waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568d2e92-cc4d-4ba5-a1a2-eb81b99b8312",
   "metadata": {},
   "outputs": [],
   "source": [
    "accdata_sim = np.load('./LF_sim_data/accwf15_combined_kumamoto_sim_yue.npy')\n",
    "accdata_bb = np.load('./LF_sim_data/accwf15_combined_kumamoto_bb_yue.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3333e7-8cdd-4aec-b4b1-17951c8de3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(accdata_sim[2,:,1])\n",
    "plt.plot(accdata_bb[16,:,1]/100)\n",
    "plt.plot(accdata_sim[16,:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b80fe6a-2cb7-41a6-989b-30db65304972",
   "metadata": {},
   "outputs": [],
   "source": [
    "accdata_bb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ea9327-1a43-433f-bf7c-0d08e0f6f842",
   "metadata": {},
   "source": [
    "## perform filter and downsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183a07ea-7c61-4af3-8815-c1e065419f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import butter, filtfilt\n",
    "\n",
    "# Get the shape of downsampled_data\n",
    "num_samples, length, num_channels = accdata_bb.shape\n",
    "\n",
    "# Initialize an array to store the filtered data\n",
    "filtered_databb = np.zeros_like(accdata_bb)\n",
    "\n",
    "# filtered_data3 = np.zeros_like(noisy_data1)\n",
    "# Sampling interval\n",
    "dt = 0.01\n",
    "sampling_rate = 1 / dt\n",
    "\n",
    "# Design a 4th-order Butterworth low-pass filter with a cutoff of 1.5 Hz\n",
    "cutoff_freq = 0.50 # Cutoff frequency in Hz\n",
    "\n",
    "# cutoff_freq2 = 1  # Cutoff frequency in Hz\n",
    "\n",
    "nyquist_freq = 0.5 * sampling_rate  # Nyquist frequency\n",
    "\n",
    "normalized_cutoff = cutoff_freq / nyquist_freq\n",
    "\n",
    "# normalized_cutoff2 = cutoff_freq2 / nyquist_freq\n",
    "\n",
    "b, a = butter(N=4, Wn=normalized_cutoff, btype='low', analog=False)\n",
    "# b1, a1 = butter(N=4, Wn=normalized_cutoff1, btype='low', analog=False)\n",
    "# b2, a2 = butter(N=4, Wn=normalized_cutoff2, btype='low', analog=False)\n",
    "# Apply the filter to each slice of the data\n",
    "for i in range(num_samples):\n",
    "    for j in range(num_channels):\n",
    "        filtered_databb[i, :, j] = lfilter(b, a, accdata_bb[i, :, j])\n",
    "        # filtered_data2[i, :, j] = filtfilt(b1, a1, noisy_data1[i, :, j])\n",
    "        # filtered_data3[i, :, j] = filtfilt(b2, a2, noisy_data1[i, :, j])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df79a23-35af-4dc5-999a-1d2ba45cba2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import butter, filtfilt\n",
    "\n",
    "# Get the shape of downsampled_data\n",
    "num_samples, length, num_channels = accdata_sim.shape\n",
    "\n",
    "# Initialize an array to store the filtered data\n",
    "filtered_data = np.zeros_like(accdata_sim)\n",
    "\n",
    "# filtered_data3 = np.zeros_like(noisy_data1)\n",
    "# Sampling interval\n",
    "dt = 0.01\n",
    "sampling_rate = 1 / dt\n",
    "\n",
    "# Design a 4th-order Butterworth low-pass filter with a cutoff of 1.5 Hz\n",
    "cutoff_freq = 0.5  # Cutoff frequency in Hz\n",
    "\n",
    "# cutoff_freq2 = 1  # Cutoff frequency in Hz\n",
    "\n",
    "nyquist_freq = 0.5 * sampling_rate  # Nyquist frequency\n",
    "\n",
    "normalized_cutoff = cutoff_freq / nyquist_freq\n",
    "\n",
    "# normalized_cutoff2 = cutoff_freq2 / nyquist_freq\n",
    "\n",
    "b, a = butter(N=4, Wn=normalized_cutoff, btype='low', analog=False)\n",
    "# b1, a1 = butter(N=4, Wn=normalized_cutoff1, btype='low', analog=False)\n",
    "# b2, a2 = butter(N=4, Wn=normalized_cutoff2, btype='low', analog=False)\n",
    "# Apply the filter to each slice of the data\n",
    "for i in range(num_samples):\n",
    "    for j in range(num_channels):\n",
    "        filtered_data[i, :, j] = lfilter(b, a, accdata_sim[i, :, j])\n",
    "        # filtered_data2[i, :, j] = filtfilt(b1, a1, noisy_data1[i, :, j])\n",
    "        # filtered_data3[i, :, j] = filtfilt(b2, a2, noisy_data1[i, :, j])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591fdadf-9b4e-4498-8853-e3debcc0175e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsample the signal from 100 Hz to 10 Hz\n",
    "downsample_factor = 10\n",
    "downsampled_data = filtered_data[:, ::downsample_factor]*100\n",
    "downsampled_databb = filtered_databb[:, ::downsample_factor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdaca9e-e314-4d7e-9040-8a96607744ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(filtered_data[0,:,2])\n",
    "id = 12\n",
    "ic = 2\n",
    "plt.plot(downsampled_data[id,:,ic])\n",
    "plt.plot(downsampled_databb[id,:,ic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1599d94-563a-44f9-8e88-b8503aeaee8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions[id,0,ic],y_test[id,0,ic]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984441d7-a705-4e94-b5e6-ffaaa924098b",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = np.concatenate( (np.arange(0.05, 0.1, 0.05) , np.arange (0.1, 1, 0.1)  , np.arange (1, 15.25, 0.25) ) ) # Time vector for the spectral response\n",
    "T = np.concatenate((np.array([0]), T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3335872f-e406-4f4d-88c5-595dcb4cfdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "downsampled_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930cbd89-00b1-40c9-b6a3-159abe7720c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remaining 10% for test set\n",
    "downsample_data_10 = downsampled_databb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dac3f3-0c0c-4ee0-a87d-7b25860bbc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "downsample_data_10.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b46f33-7584-4a7d-9ed4-e5363abb94ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform STFT to all data\n",
    "# Example data dimensions\n",
    "num_events ,time_points ,num_channels = downsample_data_10.shape\n",
    "\n",
    "\n",
    "# Define STFT parameters\n",
    "sample_rate = 10  # Hz\n",
    "nperseg = 50  # Length of each segment\n",
    "noverlap = 35  # Number of points to overlap between segments\n",
    "nfft = 100  # Number of points in the FFT\n",
    "\n",
    "# Initialize array to store STFT results\n",
    "X_low_freq_test = np.zeros((num_events,  11, 55, num_channels), dtype=np.float64)\n",
    "\n",
    "\n",
    "# Perform STFT for each event\n",
    "for event_idx in range(num_events):\n",
    "    for channel_idx in range(num_channels):\n",
    "        _, _, Zxx = stft(downsample_data_10[event_idx, :, channel_idx], fs=sample_rate, nperseg=nperseg, noverlap=noverlap, nfft=nfft)\n",
    "        X_low_freq_test[event_idx, :, :, channel_idx] = np.abs(Zxx)[0:11,:]\n",
    "\n",
    "# Verify the shape of stft_results\n",
    "print(\"Shape of STFT results for test:\", X_low_freq_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75556d64-64dd-411f-88d4-5ac5f9964538",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_stft,t_stft,Zxx = stft(downsample_data_10[event_idx, :, channel_idx], fs=sample_rate, nperseg=nperseg, noverlap=noverlap, nfft=nfft)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a035eb93-405a-4cf5-a73d-0bebce9cf543",
   "metadata": {},
   "source": [
    "### metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d96280-fd50-434c-b91e-237f2491371b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metadata = pd.read_csv('./LF_sim_data/filtered_data_kumamoto.csv',low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ae5d13-81d2-4b0a-8ae5-c7680b41ee7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metadata.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051bb143-3c56-4a61-9229-682e1c2e3514",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata['log10_PGAEW2'] = np.log10(metadata['PGAEW2(gal)'].values)\n",
    "metadata[['station longitude', 'station latitude', 'log10_PGAEW2']].to_csv('true_pga_EW_kumamoto.txt', sep=' ', header=False, index=False)\n",
    "\n",
    "metadata['log10_PGAUD2'] = np.log10(metadata['PGAUD2(gal)'].values)\n",
    "metadata[['station longitude', 'station latitude', 'log10_PGAUD2']].to_csv('true_pga_UD_kumamoto.txt', sep=' ', header=False, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50c3f7b-b26e-467f-930c-fa1297978afe",
   "metadata": {},
   "source": [
    "### preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb53ad86-20ee-48c2-9230-7d2db07c06f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = np.concatenate( (np.arange(0.05, 0.1, 0.05) , np.arange (0.1, 1, 0.1)  , np.arange (1, 15.25, 0.25) ) ) # Time vector for the spectral response\n",
    "T = np.concatenate((np.array([0]), T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c430aedd-8e9f-4f8d-a445-8d696b4ff746",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_meta = metadata[['PGAEW2(gal)', 'PGANS2(gal)','PGAUD2(gal)', \n",
    "                   'SAEW2(gal)', 'SANS2(gal)', 'SAUD2(gal)',\n",
    "                  ]].values\n",
    "X_meta_sim= metadata[['PGAEW2(gal)', 'PGANS2(gal)','PGAUD2(gal)', \n",
    "                   'SA_Vx', 'SA_Vy', 'SA_Vz'\n",
    "                  ]].values\n",
    "X_meta2 = metadata[['Vs30','JMA_Magnitude_','repi_1']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54646065-8d2b-4ad5-b8f2-e0f2a5071bc8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_meta2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0b11da-273a-4222-8789-729caf8a39d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(X_meta2[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11acb07-a0f7-49b9-95d6-fdc3352ba927",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "# 初始化 StandardScaler\n",
    "with open('./LF_sim_data/scaler.pkl', 'rb') as f:\n",
    "    scaler = pickle.load(f)\n",
    "\n",
    "# 对数据进行标准化\n",
    "X_meta2 = scaler.transform(X_meta2)\n",
    "X_meta2 = np.repeat(X_meta2[:, :, np.newaxis], 3, axis=2)\n",
    "# # normalized_data 现在是均值为 0，方差为 1 的数据\n",
    "print(X_meta2.mean(axis=0))  # 每列的均值接近 0\n",
    "print(X_meta2.std(axis=0))   # 每列的标准差接近 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c52ea44-2695-4370-9d59-2b28f892f6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设 X_meta 的形状为 (14786, 6)\n",
    "X_meta_converted = np.empty_like(X_meta, dtype=object)  # 创建与 X_meta 相同形状的空数组\n",
    "\n",
    "# 遍历 X_meta 中的每个元素，检查其类型\n",
    "for i in range(X_meta.shape[0]):  # 遍历第一维\n",
    "    for j in range(X_meta.shape[1]):  # 遍历第二维\n",
    "        item = X_meta[i, j]\n",
    "        \n",
    "        # 检查元素是否为字符串\n",
    "        if isinstance(item, str):\n",
    "            # 去除方括号并将其转换为数组（如果有空格分隔的数值）\n",
    "            X_meta_converted[i, j] = np.fromstring(item.strip('[]'), sep=' ')\n",
    "        else:\n",
    "            # 如果是浮点数，直接复制\n",
    "            X_meta_converted[i, j] = item\n",
    "\n",
    "# 将处理后的结果转换为最终的 numpy 数组\n",
    "X_meta_converted = np.array(X_meta_converted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3c0549-6c5b-47d2-bd9c-d734f377eaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设 X_meta 的形状为 (14786, 6)\n",
    "X_meta_converted_sim = np.empty_like(X_meta_sim, dtype=object)  # 创建与 X_meta 相同形状的空数组\n",
    "\n",
    "# 遍历 X_meta 中的每个元素，检查其类型\n",
    "for i in range(X_meta_sim.shape[0]):  # 遍历第一维\n",
    "    for j in range(X_meta_sim.shape[1]):  # 遍历第二维\n",
    "        item = X_meta_sim[i, j]\n",
    "        \n",
    "        # 检查元素是否为字符串\n",
    "        if isinstance(item, str):\n",
    "            # 去除方括号并将其转换为数组（如果有空格分隔的数值）\n",
    "            X_meta_converted_sim[i, j] = np.fromstring(item.strip('[]'), sep=' ')\n",
    "        else:\n",
    "            # 如果是浮点数，直接复制\n",
    "            X_meta_converted_sim[i, j] = item\n",
    "\n",
    "# 将处理后的结果转换为最终的 numpy 数组\n",
    "X_meta_converted_sim = np.array(X_meta_converted_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5479ce70-91be-4a6b-8bdc-6900c54cd0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设 X_meta 是你的数据，形状为 (14786, 6)\n",
    "# 提取第 0 列和第 3 列，沿第二个维度拼接\n",
    "col_0_3 = np.concatenate((X_meta_converted[:, 0].reshape(-1, 1), X_meta_converted[:, 3].reshape(-1, 1)), axis=1)\n",
    "\n",
    "# 提取第 1 列和第 4 列，沿第二个维度拼接\n",
    "col_1_4 = np.concatenate((X_meta_converted[:, 1].reshape(-1, 1), X_meta_converted[:, 4].reshape(-1, 1)), axis=1)\n",
    "\n",
    "# 提取第 2 列\n",
    "col_2_5 = np.concatenate((X_meta_converted[:, 2].reshape(-1, 1), X_meta_converted[:, 5].reshape(-1, 1)), axis=1)\n",
    "\n",
    "\n",
    "# 最终拼接为 (14786, 3) 数组\n",
    "result = np.concatenate((col_0_3, col_1_4, col_2_5), axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648a98ab-dc98-412e-95d6-ce7e39eff0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_0_3_sim = np.concatenate((X_meta_converted_sim[:, 0].reshape(-1, 1), X_meta_converted_sim[:, 3].reshape(-1, 1)), axis=1)\n",
    "\n",
    "# 提取第 1 列和第 4 列，沿第二个维度拼接\n",
    "col_1_4_sim = np.concatenate((X_meta_converted_sim[:, 1].reshape(-1, 1), X_meta_converted_sim[:, 4].reshape(-1, 1)), axis=1)\n",
    "\n",
    "# 提取第 2 列\n",
    "col_2_5_sim = np.concatenate((X_meta_converted_sim[:, 2].reshape(-1, 1), X_meta_converted_sim[:, 5].reshape(-1, 1)), axis=1)\n",
    "\n",
    "\n",
    "# 最终拼接为 (14786, 3) 数组\n",
    "result_sim = np.concatenate((col_0_3_sim, col_1_4_sim, col_2_5_sim), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d92437-5ee3-4b81-a00a-34b44d949976",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAEW2 = np.zeros((X_meta_converted.shape[0], 67))\n",
    "SANS2 = np.zeros((X_meta_converted.shape[0], 67))\n",
    "SAUD2 = np.zeros((X_meta_converted.shape[0], 67))\n",
    "for i in range(X_meta_converted.shape[0]):\n",
    "    SAEW2[i,:]=X_meta_converted[:, 3][i]\n",
    "    SANS2[i,:]=X_meta_converted[:, 4][i]\n",
    "    SAUD2[i,:]=X_meta_converted[:, 5][i]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28214a9d-e852-478d-9b84-6938d569fd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAEW2_sim = np.zeros((X_meta_converted_sim.shape[0], 67))\n",
    "SANS2_sim = np.zeros((X_meta_converted_sim.shape[0], 67))\n",
    "SAUD2_sim = np.zeros((X_meta_converted_sim.shape[0], 67))\n",
    "for i in range(X_meta_converted_sim.shape[0]):\n",
    "    SAEW2_sim[i,:]=X_meta_converted_sim[:, 3][i]\n",
    "    SANS2_sim[i,:]=X_meta_converted_sim[:, 4][i]\n",
    "    SAUD2_sim[i,:]=X_meta_converted_sim[:, 5][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008f9512-09a2-4d30-be72-9cd9985bd0ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "col_0_3 = np.concatenate((X_meta_converted[:, 0].reshape(-1, 1), SAEW2 ), axis=1)\n",
    "\n",
    "# 提取第 1 列和第 4 列，沿第二个维度拼接\n",
    "col_1_4 = np.concatenate((X_meta_converted[:, 1].reshape(-1, 1), SANS2), axis=1)\n",
    "\n",
    "# 提取第 2 列\n",
    "col_2_5 = np.concatenate((X_meta_converted[:, 2].reshape(-1, 1), SAUD2), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "result = np.stack((col_0_3, col_1_4, col_2_5), axis=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678e57c3-2e65-4642-a16e-66f864ece469",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_0_3_sim = np.concatenate((X_meta_converted_sim[:, 0].reshape(-1, 1), SAEW2_sim ), axis=1)\n",
    "\n",
    "# 提取第 1 列和第 4 列，沿第二个维度拼接\n",
    "col_1_4_sim = np.concatenate((X_meta_converted_sim[:, 1].reshape(-1, 1), SANS2_sim), axis=1)\n",
    "\n",
    "# 提取第 2 列\n",
    "col_2_5_sim = np.concatenate((X_meta_converted_sim[:, 2].reshape(-1, 1), SAUD2_sim), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "result_sim = np.stack((col_0_3_sim, col_1_4_sim, col_2_5_sim), axis=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2f86e3-4fd7-45e7-92f5-212b344d9bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.shape,result_sim.shape,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce979c3e-98a9-4c5f-8bfc-7db849e186f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = np.array(result, dtype=float)\n",
    "result_sim = np.array(result_sim, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d5cb88-bbb4-4af7-835c-de0d67c8b5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = np.concatenate( (np.arange(0.05, 0.1, 0.05) , np.arange (0.1, 1, 0.1)  , np.arange (1, 15.25, 0.25) ) ) # Time vector for the spectral response\n",
    "T = np.concatenate((np.array([0]), T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578a3bdb-a60c-4757-85d0-c2ebef08668e",
   "metadata": {},
   "outputs": [],
   "source": [
    "id=10\n",
    "ic=0\n",
    "plt.plot(T[47:],np.log(result_sim[id,47:,ic]*100))\n",
    "plt.plot(T[47:],np.log(result[id,47:,ic]))\n",
    "# plt.plot(T[20:],np.log(result_sim[id,20:,ic]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d53ba5d-04f5-4247-a407-d181f97b27d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(downsampled_data[id,:,ic])\n",
    "plt.plot(downsampled_databb[id,:,ic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a8406f-e26d-453c-9a9c-664fa9da5e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_meta = metadata[['Mo_','rjb_0','Vs30','Rake_','MT_Depth_','PGA1hz(gal)','PGV1hz(cm/s)',\n",
    "#                    'PGA1hEW2(gal)', 'PGA1hNS2(gal)','PGA1hUD2(gal)',\n",
    "#                    'PGV1hzEW2(cm/s)', 'PGV1hzNS2(cm/s)', 'PGV1hzUD2(cm/s)']].values\n",
    "\n",
    "# X_meta = metadata[['PGA1hEW2(gal)', 'PGA1hNS2(gal)','PGA1hUD2(gal)', \n",
    "#                    'SAEW2(gal)', 'SANS2(gal)', 'SAUD2(gal)',\n",
    "#                   ]].values\n",
    "\n",
    "# X_meta = metadata[['JMA_Magnitude_','repi_1','Vs30','Rake_','MT_Depth_','PGA1hz(gal)',]].values\n",
    "# X_meta = metadata[[ 'PGA1hz(gal)','PGV1hz(cm/s)']].values\n",
    "# X_meta[:,0]=(0.923)*(X_meta[:,0])+0.37\n",
    "\n",
    "# X_meta[:,-1]=np.log(X_meta[:,-1])\n",
    "# X_meta[:,-2]=np.log(X_meta[:,-2])\n",
    "# X_meta[:,-3]=np.log(X_meta[:,-3])\n",
    "# X_meta[:,-4]=np.log(X_meta[:,-4])\n",
    "# X_meta[:,-5]=np.log(X_meta[:,-5])\n",
    "# X_meta[:,-6]=np.log(X_meta[:,-6])\n",
    "# X_meta[:,-1]=np.log(X_meta[:,-1].astype(np.float64))\n",
    "\n",
    "# X_meta =  np.log(result[:,12:,:].astype(np.float64))\n",
    "X_meta =np.log(result[:,15:,:]).astype(np.float64)\n",
    "X_meta_sim =np.log(result_sim[:,15:,:]*100).astype(np.float64)\n",
    "\n",
    "X_meta = np.concatenate((X_meta, X_meta2.astype(np.float64)), axis=1)\n",
    "X_meta_sim  = np.concatenate((X_meta_sim , X_meta2.astype(np.float64)), axis=1)\n",
    "\n",
    "\n",
    "# y = metadata[['PGA(gal)','PGV(cm/s)']].values\n",
    "# y = metadata[['PGAEW2(gal)', 'PGANS2(gal)','PGAUD2(gal)']].values\n",
    "y = np.log(result[:,:15,:]).astype(np.float64)\n",
    "# y[:,0]=np.log(y[:,0].astype(np.float64))\n",
    "# y[:,1]=np.log(y[:,1].astype(np.float64))\n",
    "# y[:,2]=np.log(y[:,2].astype(np.float64))\n",
    "#y[:,0]=y[:,0].astype(np.float64)\n",
    "#y[:,1]=y[:,1].astype(np.float64)\n",
    "#y[:,2]=y[:,2].astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31690a23-38e4-4745-9617-004380bb0d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_meta_max = np.max(X_meta, axis=(1, 2,), keepdims=True)\n",
    "# X_meta /= X_meta_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbe8f8d-d386-47e2-9f6c-253949dbbcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_meta_test  =X_meta_sim\n",
    "X_meta_test  =X_meta_sim\n",
    "wf_test  = np.concatenate((downsample_data_10,), axis=0)\n",
    "y_test = y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f7011b-61b3-4cd1-913c-138dd328ca95",
   "metadata": {},
   "source": [
    "### normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace200f1-38bc-491c-acf4-e4c4910bc5a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "X_low_freq_test_0_2 = np.max(X_low_freq_test, axis=(1, 2, 3), keepdims=True)\n",
    "wf_test_0_2 = np.max(wf_test, axis=(1, 2, ), keepdims=True)\n",
    "# X_low_freq_test_3_6 = np.max(X_low_freq_test[:, :, :, 3:6], axis=(1, 2, 3), keepdims=True)\n",
    "\n",
    "\n",
    "X_low_freq_test /= X_low_freq_test_0_2\n",
    "wf_test /= wf_test_0_2\n",
    "# X_low_freq_test[:, :, :, 3:6] /= X_low_freq_test_3_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f24295-09ac-4aba-86bf-836b78214546",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_low_freq_test = torch.tensor(X_low_freq_test, dtype=torch.float32)\n",
    "X_meta_test = torch.tensor(X_meta_test, dtype=torch.float32)\n",
    "wf_test = torch.tensor(wf_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0c6047-befe-4aff-90b5-65fe93db6440",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f34839e-32d9-47d3-87e2-47b6b4dd768b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "from torch.optim.lr_scheduler import ExponentialLR,StepLR,CosineAnnealingLR\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class ResidualBlock1d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1,dropout_prob=0.8):\n",
    "        super(ResidualBlock1d, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        # self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        self.stride = stride\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "        \n",
    "        # Adjusting dimensions if stride is not 1\n",
    "        self.downsample = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = F.leaky_relu(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        identity = self.downsample(identity)\n",
    "        \n",
    "        out += identity\n",
    "        out = F.leaky_relu(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class ResNet1D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet1D, self).__init__()\n",
    "        # self.in_channels = 6\n",
    "        self.conv1 = nn.Conv1d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=5, stride=2, padding=1)\n",
    "        \n",
    "        self.layer1 = self._make_layer(64, 128, blocks=2, stride=2)\n",
    "        self.layer2 = self._make_layer(128, 256, blocks=2, stride=2)\n",
    "        self.layer3 = self._make_layer(256, 128, blocks=2, stride=2)\n",
    "        self.layer4 = self._make_layer(128, 64, blocks=2, stride=2)\n",
    "\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(3)\n",
    "        # self.fc = nn.Linear(256, 2)\n",
    "\n",
    "    def _make_layer(self, in_channels, out_channels, blocks, stride):\n",
    "        layers = []\n",
    "        layers.append(ResidualBlock1d(in_channels, out_channels, stride))\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(ResidualBlock1d(out_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        #print(x.shape)\n",
    "        x = self.conv1(x)\n",
    "        #print(x.shape)\n",
    "        x = self.bn1(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        #print(x.shape)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        #print(x.shape)\n",
    "\n",
    "        x = self.layer2(x)\n",
    "        #print(x.shape)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        #print(x.shape)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        #print(x.shape)\n",
    "        # x = torch.flatten(x, 1)\n",
    "        # print(x.shape)\n",
    "        # x = self.fc(x)\n",
    "        # print(x.shape)\n",
    "        self.fc_output = x\n",
    "        return x\n",
    "\n",
    "# # Example usage\n",
    "# model_wf = ResNet1D()\n",
    "# model_freq = ResNet1D()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ResNetBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, kernel_size=3, padding=1, dropout_prob=0.8):\n",
    "        super(ResNetBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size, stride=1, padding=padding)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "\n",
    "        # Adjusting dimensions if stride is not 1\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.shortcut(x)\n",
    "        out = F.leaky_relu(self.bn1(self.conv1(x)))\n",
    "        out = self.dropout(out)\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += residual\n",
    "        out = F.leaky_relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, num_blocks=[3, 3, 3, 3, 3], dropout_prob=0.8):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 64  # Initial number of channels after first convolution\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(128, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(256, num_blocks[1], stride=1)\n",
    "        self.layer3 = self._make_layer(256, num_blocks[2], stride=1)\n",
    "        self.layer4 = self._make_layer(256, num_blocks[3], stride=1)\n",
    "        self.layer5 = self._make_layer(256, num_blocks[3], stride=1)\n",
    "        self.layer6 = self._make_layer(256, num_blocks[3], stride=1)\n",
    "        self.layer7 = self._make_layer(64, num_blocks[3], stride=1)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 3))\n",
    "\n",
    "    def _make_layer(self, out_channels, num_blocks, stride):\n",
    "        layers = []\n",
    "        layers.append(ResNetBlock(self.in_channels, out_channels, stride=stride))\n",
    "        self.in_channels = out_channels\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(ResNetBlock(out_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        x = x.permute(0, 3, 1, 2)  # (batch_size, 101, 81, 3) -> (batch_size, 3, 101, 81)\n",
    "        # print(x.shape)\n",
    "        # print(f\"Input size: {x.size()}\")\n",
    "\n",
    "        x = self.pool1(F.leaky_relu(self.bn1(self.conv1(x))))\n",
    "        # print(f\"After pool1: {x.size()}\")\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        #print(f\"After layer1: {x.size()}\")\n",
    "\n",
    "        x = self.layer2(x)\n",
    "        #print(f\"After layer2: {x.size()}\")\n",
    "\n",
    "        x = self.layer3(x)\n",
    "        #print(f\"After layer3: {x.size()}\")\n",
    "\n",
    "        x = self.layer4(x)\n",
    "        #print(f\"After layer4: {x.size()}\")\n",
    "\n",
    "        x = self.layer5(x)\n",
    "        # print(f\"After layer5: {x.size()}\")\n",
    "        x = self.layer6(x)\n",
    "        x = self.layer7(x)\n",
    "        #print(f\"After avgpool: {x.size()}\")\n",
    "        x = self.avgpool(x).squeeze(2)\n",
    "        # print(f\"After avgpool: {x.size()}\")\n",
    "\n",
    "        # x = x.view(x.size(0), -1)\n",
    "        # print(f\"Flattened size: {x.size()}\")\n",
    "\n",
    "        self.fc_output = x\n",
    "        return x\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, model, model_wf, dim_meta):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.model_wf = model_wf\n",
    "        self.cnn_lstm_model = model\n",
    "        cnn_output_size = model.fc_output.size(1)  # Get the size of CNN output\n",
    "        \n",
    "        # Define Conv1D and batch normalization layers for meta data\n",
    "        self.conv1_meta = nn.Conv1d(3, 32, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.conv2_meta = nn.Conv1d(32, 64, kernel_size=3, stride=1, padding=2, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.conv3_meta = nn.Conv1d(64, 128, kernel_size=3, stride=1, padding=2, bias=False)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.conv4_meta = nn.Conv1d(128, 64, kernel_size=3, stride=1, padding=2, bias=False)\n",
    "        self.bn4 = nn.BatchNorm1d(64)\n",
    "        self.conv5_meta = nn.Conv1d(64, 32, kernel_size=3, stride=1, padding=2, bias=False)\n",
    "        self.bn5 = nn.BatchNorm1d(32)\n",
    "        self.conv6_meta = nn.Conv1d(32, 3, kernel_size=3, stride=1, padding=2, bias=False)\n",
    "        \n",
    "        self.avgpool_meta = nn.AdaptiveAvgPool1d((12, ))\n",
    "\n",
    "        # Dropout and fully connected layers\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        # self.fc1 = nn.Linear(64 + 64 + 12, 64)\n",
    "        # conv1_final = nn.Conv1d(in_channels=64 + 64 + 12, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(64 + 64 + 12 + 3, 64)\n",
    "        self.fc2 = nn.Linear(64, 15)\n",
    "        # self.fc3 = nn.Linear(64, 32)\n",
    "        # self.fc4 = nn.Linear(32, 28)\n",
    "       \n",
    "\n",
    "        # Batch normalization layers\n",
    "        # self.bn1 = nn.BatchNorm1d(512)\n",
    "        # self.bn2 = nn.BatchNorm1d(256)\n",
    "        # self.bn3 = nn.BatchNorm1d(128)\n",
    "        # self.bn4 = nn.BatchNorm1d(32)\n",
    "    \n",
    "        # # Define fully connected layers\n",
    "        # self.fc1 = nn.Linear(cnn_output_size + 64, 16)\n",
    "        # self.fc2 = nn.Linear(16, 3)\n",
    "        # # # self.fc3 = nn.Linear(256, 128)\n",
    "        # # # self.fc4 = nn.Linear(128, 32)\n",
    "        # self.fc2 = nn.Linear(6, 3)\n",
    "        \n",
    "    def forward(self, low_freq_data, wf_data , meta_data, ):\n",
    "        \n",
    "        wf_output =  self.model_wf(wf_data).permute(0, 2, 1)\n",
    "        cnn_output = self.cnn_lstm_model(low_freq_data).permute(0, 2, 1)\n",
    "        \n",
    "        # meta_data_max = meta_data_max.view(meta_data_max.size(0), -1)\n",
    "        \n",
    "        meta_x = meta_data[:,:-3,:].permute(0, 2, 1)\n",
    "        meta_x = F.leaky_relu(self.bn1(self.conv1_meta(meta_x)))\n",
    "        meta_x = self.dropout(meta_x)\n",
    "        meta_x = F.leaky_relu(self.bn2(self.conv2_meta(meta_x)))\n",
    "        meta_x = self.dropout(meta_x)\n",
    "        meta_x = F.leaky_relu(self.bn3(self.conv3_meta(meta_x)))\n",
    "        meta_x = self.dropout(meta_x)\n",
    "        meta_x = F.leaky_relu(self.bn4(self.conv4_meta(meta_x)))\n",
    "        meta_x = self.dropout(meta_x)\n",
    "        meta_x = F.leaky_relu(self.bn5(self.conv5_meta(meta_x)))\n",
    "        meta_x = self.dropout(meta_x)\n",
    "        meta_x = F.leaky_relu(self.conv6_meta(meta_x))\n",
    "        meta_x = self.avgpool_meta(meta_x)\n",
    "        # print(f\"wf_output: {wf_output.size()}\")\n",
    "        # print(f\"wf_output: {cnn_output.size()}\")\n",
    "        # print(f\"wf_output: {meta_x.size()}\")\n",
    "        # print(f\"meta_data: {meta_data[:,1,:].unsqueeze(-1).size()}\")\n",
    "        \n",
    "        combined = torch.cat((wf_output, cnn_output, meta_x, meta_data[:,-3:,:].permute(0, 2, 1)), dim=2)\n",
    "        x = F.leaky_relu(self.fc1(combined))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        return x.permute(0, 2, 1)\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8747854-9daa-4741-9b04-c623cb742df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import model_3Copy1Copy1Copy1Copy1Copy1Copy1Copy1\n",
    "resnet_model = ResNet()\n",
    "# Generate random input tensor for testing\n",
    "input_tensor = torch.randn(2, 11, 55, 3)  # (batch_size, height, width, channels)\n",
    "\n",
    "# Forward pass to see output sizes\n",
    "output = resnet_model(input_tensor)\n",
    "print(f'Final output size: {output.size()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffca1147-e43f-4648-9f76-c71a20fcbee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import model_3Copy1Copy1Copy1Copy1Copy1Copy1Copy1\n",
    "wf_model = ResNet1D()\n",
    "# Generate random input tensor for testing\n",
    "input_tensor = torch.randn(2, 800, 3)  # (batch_size, height, width, channels)\n",
    "\n",
    "# Forward pass to see output sizes\n",
    "output = wf_model(input_tensor)\n",
    "print(f'Final output size: {output.size()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d4f0d5-78c7-4b68-a4aa-c12c2d40d9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_model = CombinedModel(resnet_model,wf_model, 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010a8cd1-c10f-4586-8045-06c16200aa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights = torch.load('best_model_allmeta_3comp_mse_sgd.pth')\n",
    "\n",
    "# weights = torch.load('../compare_dataaugment/best_model_05_noagu.pth')\n",
    "weights = torch.load('./models/best_model_05.pth')\n",
    "# weights = torch.load('../compare_dataaugment/best_model_05_noaguall.pth')\n",
    "new_weights = {}\n",
    "for k, v in weights.items():\n",
    "    if k.startswith('module.'):\n",
    "        new_weights[k[7:]] = v\n",
    "    else:\n",
    "        new_weights[k] = v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd325fa-c8c0-498f-8358-e5dfbf87aa77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the saved model state dictionary\n",
    "combined_model.load_state_dict(new_weights)\n",
    "\n",
    "# Set the model to evaluation mode (this is important for certain layers like dropout and batchnorm)\n",
    "combined_model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e890874-290f-46a1-9cdf-9e7e434f7ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_low_freq_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5772082-9c7f-4143-9ddf-b003fd021a4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Assuming combined_model, X_low_freq_test, X_meta_test are defined\n",
    "\n",
    "# 1. Set model to evaluation mode\n",
    "combined_model.eval()\n",
    "\n",
    "# 2. Move test data to CPU if they were previously on GPU (assuming they are already tensors)\n",
    "\n",
    "# 3. Forward pass to get predictions\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs_test = combined_model(X_low_freq_test,wf_test, X_meta_test,)\n",
    "    # outputs_valid = combined_model(X_low_freq_valid, X_meta_test,wf_valid)\n",
    "\n",
    "# 4. Post-process predictions (assuming regression, adjust as needed)\n",
    "predictions = outputs_test.squeeze().numpy()\n",
    "\n",
    "# Now predictions are on CPU and can be used as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bee766-c6c6-4c12-8882-5c6a5f4e1296",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(predictions[3,:,1])\n",
    "plt.plot(  y_test[3,:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c2d849-76d4-410a-a207-102ea50535e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = metadata[['rjb_1']].values\n",
    "rjb = metadata[['rjb_1']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15b20a0-0f7e-4407-945d-8350174516dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.where(dist < 200)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae8a0aa-80b0-48fc-9009-00052bf90ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91597ce-473c-4339-981c-44aad160c6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431a8e74-286f-41e9-ad98-0a2fd8f8fa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Assuming you already have defined y_test, predictions, and residuals as per your example\n",
    "ind = 2\n",
    "# Assuming you already have defined y_test, predictions, and residuals as per your example\n",
    "y_testcpu = y_test[indices,0,ind]\n",
    "prediction_xgb = predictions[indices,0,ind]\n",
    "r2 = r2_score(y_testcpu, prediction_xgb)\n",
    "\n",
    "# Calculate residuals (errors)\n",
    "residuals = y_testcpu.squeeze() - prediction_xgb\n",
    "\n",
    "# Set up colors based on residuals (using the 'coolwarm' colormap with increased brightness)\n",
    "norm = plt.Normalize(-1, 1)  # Normalize colors from -4 to 4\n",
    "cmap = plt.cm.bwr  # Choose 'coolwarm' colormap for residuals\n",
    "\n",
    "plt.figure(figsize=(12/2.54, 12/2.54))\n",
    "scatter = plt.scatter(y_testcpu.squeeze(), prediction_xgb, c=residuals, cmap=cmap, s=30, norm=norm, edgecolors='k', linewidths=0.5, alpha=1)  # Plot with color mapped to residuals\n",
    "\n",
    "cbar = plt.colorbar(scatter, label='Residuals')  # Add color bar showing the range of residuals\n",
    "cbar.ax.tick_params(labelsize=10)  # Adjust color bar tick size for readability\n",
    "\n",
    "# plt.plot([-6, 5], [-6, 5], color='k', linestyle='--', linewidth=0.6)  # Plot thin y=x line\n",
    "\n",
    "plt.xlabel('True values')\n",
    "plt.ylabel('Predicted values')\n",
    "\n",
    "# Add R2 score as a label on the plot\n",
    "plt.text(-2, 3.5, f'R2 = {r2:.2f}', fontsize=12, color='black', ha='left', va='top')\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "plt.gca().set_aspect('equal')\n",
    "\n",
    "major_ticks = np.arange(-10, 8, 2)  # Major ticks from -10 to 5 with step 2\n",
    "minor_ticks = np.arange(-10, 8, 1)  # Minor ticks from -10 to 5 with step 1\n",
    "\n",
    "plt.gca().set_xticks(major_ticks)\n",
    "plt.gca().set_xticks(minor_ticks, minor=True)\n",
    "plt.gca().set_yticks(major_ticks)\n",
    "plt.gca().set_yticks(minor_ticks, minor=True)\n",
    "# plt.xlim([-7, 4])\n",
    "# plt.ylim([-7, 4])\n",
    "\n",
    "# plt.savefig('R2_resnet.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4ebb14-ad57-4cb4-b17f-ead0c684eb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(dist[indices],y_test[indices,0,ind])\n",
    "plt.scatter(dist[indices],predictions[indices,0,ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0468c54-fafb-4b8c-9d74-d3a39fac060e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_plot = metadata[['station longitude', 'station latitude', 'PGAEW2(gal)', 'PGANS2(gal)', 'PGAUD2(gal)',]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee46a3c-96f6-4fbd-a611-c48c293ed62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3a540d-7c46-49e7-b2e7-2d2934a088ad",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# np.savetxt('true_pga_kumamoto.txt', data_truepga, fmt='%.6f', delimiter=' ', newline='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca872585-b5a0-44ba-9f74-059db329babd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot observed data\n",
    "plt.scatter(rjb[indices], np.sqrt(np.exp(y_test[indices,0,ind])**2+np.exp(y_test[indices,1,ind])**2+np.exp(y_test[indices,2,ind])**2) / (9.8 * 1e2), label='Observed PGA', color='blue', marker='o')\n",
    "\n",
    "# Plot predicted data\n",
    "plt.scatter(rjb[indices],  np.sqrt(np.exp(predictions[indices,0,ind])**2+np.exp(predictions[indices,1,ind])**2+np.exp(predictions[indices,2,ind])**2) / (9.8 * 1e2), label='Predicted PGA(sim)', color='red', marker='x')\n",
    "\n",
    "# Set log scale for both axes\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "\n",
    "# Add labels and legend\n",
    "plt.xlabel('R$_{jb}$ [km]')\n",
    "plt.ylabel('PGA [g]')\n",
    "# plt.title('Observed vs Predicted vs CY14 Median PGA')\n",
    "# plt.xlim([0.1,130])\n",
    "# plt.ylim([0.001,2])\n",
    "plt.legend()\n",
    "# Show plot with grid\n",
    "plt.grid(True, which=\"both\", linestyle='--', linewidth=0.5)\n",
    "# plt.savefig('kumamoto_noaguall.png',dpi=300)\n",
    "# plt.savefig('kumamoto_noaguall.pdf')\n",
    "# plt.savefig('kumamoto_noaguall.png',dpi=300)\n",
    "# plt.savefig('kumamoto_noaguall.pdf')\n",
    "# np.savetxt('y_test_05Hz.txt',  np.exp(predictions[indices,0,ind]))\n",
    "# np.savetxt('Rjb_index.txt', rjb[indices] )\n",
    "# np.savetxt('y_test_true.txt', np.exp(y_test[indices,0,ind]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86701e5f-f9c7-4b0a-939b-2ac6d3afd06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadatasim = pd.read_csv('./LF_sim_data/filtered_data_kumamoto_yue.csv',low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a8fb4c-d80a-4b35-9a41-db1a0a437ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "PGA1UD = metadatasim['PGAUD2(gal)_x'].values.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a703795-0cc4-4560-8566-43644210421b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adc6263-ce33-4ff6-99e5-1b0687392319",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming the data is already defined (e.g., medianvec, sigma, y_test, predictions, etc.)\n",
    "\n",
    "# Set up the 2x1 figure\n",
    "fig, axs = plt.subplots(1, 2, figsize=(14, 10))  # Adjusted for two subplots\n",
    "\n",
    "# Set (a) plot\n",
    "# Observed and predicted PGA values for the first plot\n",
    "observed_pga = np.exp(y_test[indices, 0, ind]) / (9.8 * 1e2)\n",
    "predicted_pga = PGA1UD[indices] / (9.8 * 1e2)\n",
    "\n",
    "combined_data = np.vstack((rjb[indices].squeeze(), observed_pga, predicted_pga)).T\n",
    "sorted_data = combined_data[np.argsort(-combined_data[:, 1])]\n",
    "\n",
    "sorted_rjb = sorted_data[:, 0]\n",
    "sorted_observed_pga = sorted_data[:, 1]\n",
    "sorted_predicted_pga = sorted_data[:, 2]\n",
    "\n",
    "x = np.arange(len(sorted_rjb))\n",
    "axs[0].bar(x - 0.2, sorted_observed_pga, width=0.4, label='Observed PGA', color='blue')\n",
    "axs[0].bar(x + 0.2, sorted_predicted_pga, width=0.4, label='Simulated PGA (0.5Hz)', color='red')\n",
    "\n",
    "# (a) plot details\n",
    "axs[0].set_yscale('log')\n",
    "axs[0].set_xlabel('Stations (Sorted by Observed PGA)', fontsize=20)\n",
    "axs[0].set_ylabel('PGA [g]', fontsize=20)\n",
    "axs[0].legend(fontsize=20)\n",
    "axs[0].grid(True, which=\"both\", linestyle='--', linewidth=0.5)\n",
    "axs[0].set_xticks(x[::5])  # Set x-ticks to the bar positions\n",
    "axs[0].set_yticks([0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10])  # Custom y-ticks (log scale)\n",
    "axs[0].set_ylim([0.00001, 2])\n",
    "axs[0].text(-2, 1, '(a)', fontsize=22, fontweight='bold')\n",
    "axs[0].tick_params(axis='both', labelsize=16)\n",
    "# Set (b) plot\n",
    "observed_pga = np.exp(y_test[indices, 0, ind]) / (9.8 * 1e2)\n",
    "predicted_pga = np.exp(predictions[indices, 0, ind]) / (9.8 * 1e2)\n",
    "\n",
    "combined_data = np.vstack((rjb[indices].squeeze(), observed_pga, predicted_pga)).T\n",
    "sorted_data = combined_data[np.argsort(-combined_data[:, 1])]\n",
    "\n",
    "sorted_rjb = sorted_data[:, 0]\n",
    "sorted_observed_pga = sorted_data[:, 1]\n",
    "sorted_predicted_pga = sorted_data[:, 2]\n",
    "\n",
    "x = np.arange(len(sorted_rjb))\n",
    "axs[1].bar(x - 0.2, sorted_observed_pga, width=0.4, label='Observed PGA', color='blue')\n",
    "axs[1].bar(x + 0.2, sorted_predicted_pga, width=0.4, label='LF2BB PGA', color='red')\n",
    "\n",
    "# (b) plot details\n",
    "axs[1].set_yscale('log')\n",
    "axs[1].set_xlabel('Stations (Sorted by Observed PGA)', fontsize=20)\n",
    "# axs[1].set_ylabel('PGA [g]', fontsize=14)\n",
    "axs[1].legend(fontsize=20)\n",
    "axs[1].grid(True, which=\"both\", linestyle='--', linewidth=0.5)\n",
    "axs[1].set_xticks(x[::5])  # Set x-ticks to the bar positions\n",
    "axs[1].set_yticks([0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10])  # Custom y-ticks (log scale)\n",
    "axs[1].set_ylim([0.00001, 2])\n",
    "axs[1].text(-2, 1, '(b)', fontsize=22, fontweight='bold')\n",
    "axs[1].tick_params(axis='both', labelsize=16)\n",
    "# Tight layout to prevent overlapping\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot as PNG and PDF\n",
    "# plt.savefig('pga_comparison_ud.png', dpi=300)  # Save as PNG\n",
    "# plt.savefig('pga_comparison_ud.pdf', dpi=300)  # Save as PDF\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568c7b59-30ef-4876-ba8b-bdb9401663fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import gaussian_kde\n",
    "inx = 0\n",
    "ind=2\n",
    "observed_pga = np.exp(y_test[indices, inx, ind]) / (9.8 * 1e2)\n",
    "predicted_pga = np.exp(predictions[indices, inx, ind]) / (9.8 * 1e2)\n",
    "\n",
    "combined_data = np.vstack((rjb[indices].squeeze(), observed_pga, predicted_pga)).T\n",
    "sorted_data = combined_data[np.argsort(-combined_data[:, 1])]\n",
    "\n",
    "sorted_rjb = sorted_data[:, 0]\n",
    "sorted_observed_pga = sorted_data[:, 1]\n",
    "sorted_predicted_pga1 = sorted_data[:, 2]\n",
    "corr_a, pval_a = pearsonr(sorted_observed_pga, sorted_predicted_pga1)\n",
    "print(f\"(a) Pearson correlation: {corr_a:.3f}, p-value: {pval_a:.3e}\")\n",
    "\n",
    "observed_pga = np.exp(y_test[indices, 0, ind]) / (9.8 * 1e2)\n",
    "predicted_pga = PGA1UD[indices] / (9.8 * 1e2)\n",
    "\n",
    "combined_data = np.vstack((rjb[indices].squeeze(), observed_pga, predicted_pga)).T\n",
    "sorted_data = combined_data[np.argsort(-combined_data[:, 1])]\n",
    "\n",
    "sorted_rjb = sorted_data[:, 0]\n",
    "sorted_observed_pga = sorted_data[:, 1]\n",
    "sorted_predicted_pga2 = sorted_data[:, 2]\n",
    "corr_a, pval_a = pearsonr(sorted_observed_pga, sorted_predicted_pga2)\n",
    "print(f\"(a) Pearson correlation: {corr_a:.3f}, p-value: {pval_a:.3e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78673912-64c1-4879-9ad2-016819056d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors1 = (np.log10(sorted_observed_pga) - np.log10(sorted_predicted_pga1))/np.log10(sorted_observed_pga)\n",
    "errors2 = (np.log10(sorted_observed_pga) - np.log10(sorted_predicted_pga2))/np.log10(sorted_observed_pga)\n",
    "\n",
    "# errors1 = (np.log(sorted_observed_pga) - np.log(sorted_predicted_pga1))\n",
    "# errors2 = (np.log(sorted_observed_pga) - np.log(sorted_predicted_pga2))\n",
    "\n",
    "# errors1 =(sorted_observed_pga - sorted_predicted_pga1) / sorted_observed_pga\n",
    "# errors2 = (sorted_observed_pga - sorted_predicted_pga2) / sorted_observed_pga\n",
    "\n",
    "# Calculate MSE (for each station, one value)\n",
    "mse_all1 = errors1\n",
    "mse_all2 = errors2\n",
    "\n",
    "# Calculate KDE for both errors\n",
    "kde1 = gaussian_kde(errors1, bw_method=0.3)\n",
    "kde2 = gaussian_kde(errors2, bw_method=0.3)\n",
    "\n",
    "# Create a range of x values to evaluate the KDE\n",
    "x = np.linspace(min(min(errors1), min(errors2)), max(max(errors1), max(errors2)), 1000)\n",
    "\n",
    "# Plot the KDEs\n",
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "\n",
    "# Plot and fill the area under KDE for errors1 and errors2\n",
    "ax.fill_between(x, kde1(x), alpha=0.3, color='teal')  # Fill area under the KDE of errors1\n",
    "# ax.fill_between(x, kde2(x), alpha=0.3, color='orange')  # Fill area under the KDE of errors2\n",
    "\n",
    "# Plot the KDE curves on top\n",
    "ax.plot(x, kde1(x), label=\"KDE of errors1\", color='teal', linewidth=2)\n",
    "# ax.plot(x, kde2(x), label=\"KDE of errors2\", color='orange', linewidth=2)\n",
    "\n",
    "# Add labels, legend, and grid\n",
    "ax.set_xlabel(\"Error Value\", fontsize=14)\n",
    "ax.set_ylabel(\"Density\", fontsize=14)\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.xlim([-0.5, 0.5])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137da6e5-9c24-498f-9427-3561bb0555ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted_pga11 = np.log(PGA1UD[:])\n",
    "\n",
    "observed_pga = np.log10(np.exp(y_test[:, 0, ind]))\n",
    "predicted_pga1 = np.log10(PGA1UD[:])\n",
    "predicted_pga2 = np.log10(np.exp(predictions[:, 0, ind]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be031f9b-c479-4879-9867-c7d9093927ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "observed_pga.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c67a9b-6382-43b6-b75d-0ead8b3bf25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(np.array(observed_pga))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895ce345-fe5f-4f58-aa76-1c372203a16e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541649f3-d8b1-40d2-b382-c4042c06e38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算比值\n",
    "ratio1 = predicted_pga1 / observed_pga\n",
    "ratio2 = predicted_pga2 / observed_pga\n",
    "\n",
    "# 计算均值、最小值和最大值\n",
    "mean_ratio1 = torch.mean(ratio1)\n",
    "mean_ratio2 = torch.mean(ratio2)\n",
    "\n",
    "min_ratio1 = torch.min(ratio1)\n",
    "min_ratio2 = torch.min(ratio2)\n",
    "\n",
    "max_ratio1 = torch.max(ratio1)\n",
    "max_ratio2 = torch.max(ratio2)\n",
    "\n",
    "# 打印结果\n",
    "print(\"Mean Ratio 1:\", mean_ratio1.item())\n",
    "print(\"Mean Ratio 2:\", mean_ratio2.item())\n",
    "print(\"Min Ratio 1:\", min_ratio1.item())\n",
    "print(\"Min Ratio 2:\", min_ratio2.item())\n",
    "print(\"Max Ratio 1:\", max_ratio1.item())\n",
    "print(\"Max Ratio 2:\", max_ratio2.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bb0215-cf36-4afd-a351-0282f2f20099",
   "metadata": {},
   "outputs": [],
   "source": [
    "observed_pga[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a886ed-d921-4e81-bd5b-a2f545dc7d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(ratio2).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce24058-91d2-4301-855f-0ca93d1222a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_pga1 = torch.mean(abs((observed_pga - predicted_pga1))/observed_pga )\n",
    "\n",
    "\n",
    "mse_pga2 = torch.mean(abs((observed_pga - predicted_pga2))/observed_pga )\n",
    "\n",
    "# mse_pga1 = torch.mean((  predicted_pga1)/observed_pga)\n",
    "# mse_pga2 = torch.mean(( predicted_pga2)/observed_pga)\n",
    "\n",
    "\n",
    "print(f\"MSE between observed_pga and predicted_pga1: {mse_pga1.item()}\")\n",
    "print(f\"MSE between observed_pga and predicted_pga2: {mse_pga2.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deec7446-918a-4494-aca1-c9c6fdc60864",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# Initialize index variable\n",
    "ind = 2\n",
    "\n",
    "# Assuming y_test and predictions are already defined as 2D arrays or tensors\n",
    "# Create empty lists for storing residuals\n",
    "residuals_all = []\n",
    "mae_all = []\n",
    "sigma_all = []\n",
    "\n",
    "# Set a style\n",
    "# plt.style.use('seaborn-darkgrid')\n",
    "\n",
    "# Create a figure with a specific size\n",
    "\n",
    "\n",
    "# Loop over each row of y_test\n",
    "for i in range(y_test.shape[1]):\n",
    "    # Get current row values for y_test and predictions\n",
    "    y_testcpu = np.exp(y_test[indices, i, ind].squeeze())\n",
    "    prediction1 = np.exp(predictions[indices, i, ind])\n",
    "    \n",
    "    # Convert tensors to NumPy arrays if needed\n",
    "    y_testcpu_np = y_testcpu.cpu().numpy() if isinstance(y_testcpu, torch.Tensor) else y_testcpu\n",
    "    prediction_np1 = prediction1.cpu().numpy() if isinstance(prediction1, torch.Tensor) else prediction1\n",
    "    \n",
    "    # Calculate residuals\n",
    "    residual = np.log10(prediction_np1  / y_testcpu_np)\n",
    "    \n",
    "    # Store residuals\n",
    "    residuals_all.append(residual)\n",
    "    \n",
    "    # Calculate MAE and standard deviation\n",
    "    mae = np.mean(np.abs(residual**2))\n",
    "    sigma = np.std(residual)\n",
    "    \n",
    "    mae_all.append(mae)\n",
    "    sigma_all.append(sigma)\n",
    "\n",
    "# Calculate mean and standard deviation of residuals\n",
    "mean_residuals = [np.mean(residual) for residual in residuals_all]\n",
    "std_residuals = [np.std(residual) for residual in residuals_all]\n",
    "\n",
    "# Define indices for x-axis\n",
    "T = np.arange(len(mean_residuals))  # Assuming T is the range of indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9313b4-b791-4480-844b-467c30b0b006",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = np.concatenate( (np.arange(0.05, 0.1, 0.05) , np.arange (0.1, 1, 0.1)  , np.arange (1, 15.25, 0.25) ) ) # Time vector for the spectral response\n",
    "T = np.concatenate((np.array([0]), T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7d6d0e-cf42-4cf4-9a7c-c6deabab6b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "# Use error bar plot for mean and standard deviation\n",
    "# Use error bar plot for mean and standard deviation with black error bars\n",
    "plt.errorbar(T[:15], mean_residuals[:15], yerr=std_residuals[:15], fmt='o',\n",
    "             color='black', markersize=8,  label='Mean ± 1 Std Dev', \n",
    "             ecolor='black', elinewidth=2, capsize=5)\n",
    "\n",
    "# Add a black line at y=0\n",
    "plt.axhline(y=0, color='black', linewidth=2)\n",
    "\n",
    "# Enhance plot aesthetics\n",
    "plt.xlabel('T(1/s)', fontsize=16)\n",
    "plt.ylabel('$log_{10}(SA_{pred}/SA_{obs})$', fontsize=16)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)  # Dashed grid lines\n",
    "plt.ylim([-1, 1])\n",
    "plt.xticks(fontsize=16)  # Font size for x-ticks\n",
    "plt.yticks(fontsize=16)  # Font size for y-ticks\n",
    "\n",
    "# Add legend\n",
    "plt.legend(fontsize=16)\n",
    "\n",
    "# Increase border thickness\n",
    "plt.gca().spines['top'].set_linewidth(3)\n",
    "plt.gca().spines['right'].set_linewidth(3)\n",
    "plt.gca().spines['left'].set_linewidth(3)\n",
    "plt.gca().spines['bottom'].set_linewidth(3)\n",
    "\n",
    "plt.tight_layout()  # Adjust layout to prevent clipping\n",
    "# plt.savefig('errorbar_0.5Hz_kumamoto_noagu.png',dpi=300)\n",
    "plt.show()\n",
    "# np.savetxt(\"/data_array1/panyx/ANN_GMPE/resnet_20240903/figure_forpaper/figure_bbresult_whole/residual_gp2.txt\",\n",
    "#            np.column_stack((T[:11], mean_residuals[:11], std_residuals[:11])),\n",
    "#            header=\"Period Mean_Residual Std_Residual\",\n",
    "#            fmt=\"%.6f\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
